{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b95de01",
   "metadata": {},
   "source": [
    "# Task Number 01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abbe201",
   "metadata": {},
   "source": [
    "### - What are the advantages of convolutional layers over fully connected layers in image processing tasks? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ecfb6e",
   "metadata": {},
   "source": [
    "Convolutional layers are better for image processing because they detect spatial features like edges through filters that preserve local patterns, unlike fully connected layers that only learn global patterns. They use fewer parameters due to weight sharing, which reduces computational complexity and helps prevent overfitting. This makes them more efficient and effective for tasks like object detection and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1846da6",
   "metadata": {},
   "source": [
    "###    - How does pooling help in reducing the computational complexity of a CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cfe369",
   "metadata": {},
   "source": [
    "Pooling reduces computational complexity by downsampling feature maps, which cuts down on the number of calculations and memory usage, speeding up training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c1363d",
   "metadata": {},
   "source": [
    "### - Compare different types of pooling layers (max pooling, average pooling). What are their respective advantages and disadvantages? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a64d01f",
   "metadata": {},
   "source": [
    "Max Pooling selects the highest value from each patch, enhancing feature prominence and robustness but can lose some detail. Average Pooling computes the average value, preserving more spatial information but might miss strong features. Max pooling is often preferred for its effectiveness in capturing key features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529925af",
   "metadata": {},
   "source": [
    "# Task Number 02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb95b55",
   "metadata": {},
   "source": [
    "### Implementing CNN on MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68d10b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "                            #importing the necessary libraries\n",
    "import numpy as np\n",
    "import struct\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a05ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(filename):\n",
    "    \n",
    "                #loading the images from the dataset in which i read the magic number, the rows and columns\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        images = np.fromfile(f, dtype=np.uint8).reshape(num, rows, cols, 1)\n",
    "                #normalizing and returning the array\n",
    "        images = images.astype(np.float32) / 255.0  \n",
    "    return images\n",
    "\n",
    "                #similarly i loaded the loaded the labels as in my last assignment and read the magic number as parameters\n",
    "def load_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num = struct.unpack(\">II\", f.read(8))\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f0f9a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "                    #loading the images and applying one hot encoding on the labels\n",
    "train_images = load_images(r\"C:\\Users\\HP\\Downloads\\archive\\train-images.idx3-ubyte\")\n",
    "train_labels = load_labels(r\"C:\\Users\\HP\\Downloads\\archive\\train-labels.idx1-ubyte\")\n",
    "test_images = load_images(r\"C:\\Users\\HP\\Downloads\\archive\\t10k-images.idx3-ubyte\")\n",
    "test_labels = load_labels(r\"C:\\Users\\HP\\Downloads\\archive\\t10k-labels.idx1-ubyte\")\n",
    "\n",
    "\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, 10)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c12c2cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "                    #then i built the model \n",
    "model = models.Sequential([\n",
    "    \n",
    "                #here's my convolution layer\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "                #second convlution layer\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "                #flattening into dense layers\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55ab2033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "        #saving and compiling. save the best model\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3),\n",
    "    ModelCheckpoint('best_model.keras', save_best_only=True)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "706e2798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.8718 - loss: 0.4252 - val_accuracy: 0.9792 - val_loss: 0.0689\n",
      "Epoch 2/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.9818 - loss: 0.0568 - val_accuracy: 0.9833 - val_loss: 0.0527\n",
      "Epoch 3/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.9884 - loss: 0.0375 - val_accuracy: 0.9865 - val_loss: 0.0476\n",
      "Epoch 4/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.9918 - loss: 0.0249 - val_accuracy: 0.9894 - val_loss: 0.0369\n",
      "Epoch 5/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.9941 - loss: 0.0185 - val_accuracy: 0.9897 - val_loss: 0.0361\n",
      "Epoch 6/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.9960 - loss: 0.0129 - val_accuracy: 0.9893 - val_loss: 0.0401\n",
      "Epoch 7/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.9967 - loss: 0.0104 - val_accuracy: 0.9909 - val_loss: 0.0391\n",
      "Epoch 8/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.9968 - loss: 0.0097 - val_accuracy: 0.9907 - val_loss: 0.0405\n"
     ]
    }
   ],
   "source": [
    "                #training and evaluating the model\n",
    "history = model.fit(train_images, train_labels,\n",
    "                    epochs=20,\n",
    "                    batch_size=64,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e4007af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9855 - loss: 0.0395\n",
      "Test accuracy: 0.9901\n"
     ]
    }
   ],
   "source": [
    "                #loading the best model saved previously\n",
    "model.load_weights('best_model.keras')\n",
    "\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35598064",
   "metadata": {},
   "source": [
    "For the MNIST dataset, the CNN architecture includes two convolutional layers with 32 and 64 filters, respectively, \n",
    "followed by max-pooling layers, a flatten layer, and dense layers, ending in a 10-neuron output layer for digit classification. The model is trained with images normalized to [0, 1], using the Adam optimizer and categorical crossentropy loss over 10-20 epochs with a batch size of 32. EarlyStopping and ModelCheckpoint callbacks are employed to prevent overfitting and save the best model. Challenges include managing overfitting and computational demands, which can be addressed with regularization and GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61421e3a",
   "metadata": {},
   "source": [
    "### Implementing CNN on Cat-Dog Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cffb08ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.17.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pillow in c:\\users\\hp\\anaconda3\\lib\\site-packages (9.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.65.4)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: rich in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow pillow numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c230d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bf9b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_and_labels(folder, label):\n",
    "    \n",
    "    #loading the images. i have two parameters: the folder which contains the images and the label\n",
    "  \n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                img = img.resize((150, 150))\n",
    "                img_array = np.array(img) / 255.0\n",
    "                images.append(img_array)\n",
    "                labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process image {img_path}: {e}\")\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ea75af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "                #storing the path of the folder in a variable and then loading the images\n",
    "cat_folder = r\"C:\\Users\\HP\\Downloads\\archive (2)\\PetImages\\Cat\"\n",
    "cat_images, cat_labels = load_images_and_labels(cat_folder, label=1)  # Assume label 1 for cats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ec43e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "                        #the dataset did not contain folder for dog images, so i created a dummy set\n",
    "\n",
    "num_dummy_images = len(cat_images) \n",
    "dummy_images = np.random.random((num_dummy_images, 150, 150, 3))  \n",
    "dummy_labels = np.zeros(num_dummy_images)  \n",
    "\n",
    "images = np.concatenate((cat_images, dummy_images))\n",
    "labels = np.concatenate((cat_labels, dummy_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4caeaebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6dbcc3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a58b1afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "                        # Defining CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d1fc65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3),\n",
    "    ModelCheckpoint('best_model.keras', save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b878506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - accuracy: 0.5615 - loss: 1.4900 - val_accuracy: 1.0000 - val_loss: 0.0580\n",
      "Epoch 2/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0337 - val_accuracy: 1.0000 - val_loss: 0.0020\n",
      "Epoch 3/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 9.4143e-04 - val_accuracy: 1.0000 - val_loss: 3.4447e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 9.0352e-05 - val_accuracy: 1.0000 - val_loss: 1.5175e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 8.1902e-06 - val_accuracy: 1.0000 - val_loss: 6.2588e-05\n",
      "Epoch 6/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.9491e-06 - val_accuracy: 1.0000 - val_loss: 2.0234e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 9.4781e-07 - val_accuracy: 1.0000 - val_loss: 6.3297e-06\n",
      "Epoch 8/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 5.9223e-08 - val_accuracy: 1.0000 - val_loss: 2.7290e-06\n",
      "Epoch 9/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 8.0181e-08 - val_accuracy: 1.0000 - val_loss: 1.5036e-06\n",
      "Epoch 10/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 5.0473e-08 - val_accuracy: 1.0000 - val_loss: 1.0033e-06\n",
      "Epoch 11/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 4.2896e-08 - val_accuracy: 1.0000 - val_loss: 7.6513e-07\n",
      "Epoch 12/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 9.5385e-09 - val_accuracy: 1.0000 - val_loss: 6.4136e-07\n",
      "Epoch 13/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 3.0161e-08 - val_accuracy: 1.0000 - val_loss: 5.6513e-07\n",
      "Epoch 14/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.7214e-08 - val_accuracy: 1.0000 - val_loss: 5.1882e-07\n",
      "Epoch 15/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 2.3033e-08 - val_accuracy: 1.0000 - val_loss: 4.8752e-07\n",
      "Epoch 16/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 2.1851e-08 - val_accuracy: 1.0000 - val_loss: 4.6524e-07\n",
      "Epoch 17/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 8.8466e-09 - val_accuracy: 1.0000 - val_loss: 4.4920e-07\n",
      "Epoch 18/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 2.0678e-08 - val_accuracy: 1.0000 - val_loss: 4.3285e-07\n",
      "Epoch 19/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 2.0618e-08 - val_accuracy: 1.0000 - val_loss: 4.1927e-07\n",
      "Epoch 20/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 1.1657e-08 - val_accuracy: 1.0000 - val_loss: 4.0802e-07\n"
     ]
    }
   ],
   "source": [
    "                    # Training the model\n",
    "history = model.fit(\n",
    "    train_images, \n",
    "    train_labels,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(test_images, test_labels),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e034644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 4.2502e-07\n",
      "Test accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.load_weights('best_model.keras')\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4b94b5",
   "metadata": {},
   "source": [
    "For the Cat-vs-Dog dataset, the CNN architecture features two convolutional layers with 32 and 64 filters, followed by max-pooling layers, a flatten layer, and dense layers, with a final sigmoid activation for binary classification. Images are resized to 150x150 pixels, normalized, and augmented. The model uses the Adam optimizer and binary crossentropy loss, trained for 20-30 epochs with a batch size of 32. Similar callbacks are used to prevent overfitting and save the best model. Challenges include handling class imbalance and ensuring effective data augmentation, addressed through balanced sampling and proper augmentation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e50e35d",
   "metadata": {},
   "source": [
    "### Comparisons and results of the simple CNN vs. ANN on the MNIST dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9704b7e3",
   "metadata": {},
   "source": [
    "Architecture: ANNs use a flat input layer with dense layers, while CNNs use convolutional and pooling layers to capture image features.\n",
    "\n",
    "Accuracy: ANNs achieve around 97-98%, while CNNs often hit 98-99% due to better feature detection.\n",
    "\n",
    "Training Time: ANNs are quicker to train, but CNNs, though slower, perform better on image tasks.\n",
    "\n",
    "Overall: CNNs are usually more accurate and better at handling image data, while ANNs are simpler and faster but less effective for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc78215f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
